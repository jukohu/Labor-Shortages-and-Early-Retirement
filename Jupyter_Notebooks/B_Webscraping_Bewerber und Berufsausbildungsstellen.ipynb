{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79959ef7",
   "metadata": {},
   "source": [
    "## In a similar fashion as for the Arbeitsstellen, this code scrapes the data for Berufsausbildungsstellen, Bewerber, etc. from the website of the BA\n",
    "\n",
    "Source: https://statistik.arbeitsagentur.de/SiteGlobals/Forms/Suche/Einzelheftsuche_Formular.html?nn=1459818&topic_f=ausb-ausbildungsstellenmarkt-mit-zkt\n",
    "\n",
    "Unfortunately, I didn't safe the part of the code that dowloads all Excel and Pdf files from the BA's website but I do have the folder with all those files. The code for the dowload of those files, however, can be easily replicated following the same logic as the code for the dowload of the Arbeitsstellen data from the BA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "478e79b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Settings\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max.rows', None)\n",
    "pd.set_option('display.max.columns', None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adec0513",
   "metadata": {},
   "source": [
    "Between 2020 and 2025 the data is stored in excel files from which we extract the relevant tables and merge together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b70543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract data from Excel files\n",
    "########################################################################\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Path to the folder containing Excel files\n",
    "excel_folder = r\"C:\\Users\\jhummels\\OneDrive - DIW Berlin\\Gehlen, Annica's files - retirement-labor-shortages\\Data\\BA_data\\B_Bewerber und Berufsausbildungsstellen\\arbeitsagentur_excels_Ausbildung\"\n",
    "\n",
    "# List to collect DataFrames\n",
    "all_dfs = []\n",
    "\n",
    "# Define ALL acceptable title snippets (case-insensitive, partial match)\n",
    "title_snippets = [\n",
    "    \"bewerberinnen und bewerber für berufsausbildungsstellen und berufsausbildungsstellen nach berufsbereichen und -gruppen\",\n",
    "    \"bewerberinnen und bewerber sowie betriebliche berufsausbildungsstellen nach berufen\",\n",
    "]\n",
    "\n",
    "# Loop through all Excel files in the folder\n",
    "for file in os.listdir(excel_folder):\n",
    "    if file.endswith(\".xlsx\"):\n",
    "        filepath = os.path.join(excel_folder, file)\n",
    "\n",
    "        # Extract metadata from filename\n",
    "        match = re.search(r\"ausbildungsstellenmarkt-mit-zkt-(\\d{2})-0-(\\d{6})\", file)\n",
    "        if not match:\n",
    "            print(f\"⚠️ Filename doesn't match expected pattern: {file}\")\n",
    "            continue\n",
    "\n",
    "        bundesland = match.group(1)\n",
    "        year = int(match.group(2)[:4])\n",
    "        month = int(match.group(2)[4:])\n",
    "\n",
    "        # ✅ Filter: Keep only files between November 2020 and March 2025\n",
    "        if (year < 2020) or (year == 2020 and month < 11) or (year == 2025 and month > 3) or (year > 2025):\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            xls = pd.ExcelFile(filepath)\n",
    "            sheet_names = xls.sheet_names\n",
    "\n",
    "            # Search for the correct sheet based on sheet content (first rows)\n",
    "            sheet_name = None\n",
    "            for s in sheet_names:\n",
    "                try:\n",
    "                    preview = pd.read_excel(xls, sheet_name=s, nrows=3, header=None)\n",
    "                    first_rows_text = \" \".join(preview.astype(str).fillna(\"\").values.flatten()).lower()\n",
    "\n",
    "                    if any(snippet in first_rows_text for snippet in title_snippets):\n",
    "                        sheet_name = s\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"⚠️ Could not preview sheet {s} in {file}: {e}\")\n",
    "\n",
    "            if not sheet_name:\n",
    "                print(f\"❌ No sheet with expected title found in {file}. Sheets: {sheet_names}\")\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"✅ Selected sheet by content: {sheet_name} in file: {file}\")\n",
    "            \n",
    "            # Try finding the header row by previewing the top 20 rows\n",
    "            preview = pd.read_excel(xls, sheet_name=sheet_name, nrows=12, header=None)\n",
    "\n",
    "            # Search for row that looks like a valid header (tweak logic as needed)\n",
    "            for i, row in preview.iterrows():\n",
    "                if \"1\" in str(row.values) and \"2\" in str(row.values) and \"3\" in str(row.values) and \"4\" in str(row.values):\n",
    "                    header_row = i\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"⚠️ Could not find valid header row in {file}.\")\n",
    "                continue\n",
    "\n",
    "            # Now read the full data starting from the detected header row\n",
    "            df = pd.read_excel(xls, sheet_name=sheet_name, header=header_row)\n",
    "\n",
    "            # Add metadata\n",
    "            df[\"Bundesland\"] = bundesland\n",
    "            df[\"Year\"] = year\n",
    "            df[\"Month\"] = month\n",
    "            df[\"source_file\"] = file\n",
    "\n",
    "            all_dfs.append(df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading file {file}: {e}\")\n",
    "\n",
    "# Combine all DataFrames\n",
    "if all_dfs:\n",
    "    combined_excel_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ Combined {len(all_dfs)} files. Total rows: {combined_excel_df.shape[0]}\")\n",
    "    print(combined_excel_df.head())\n",
    "else:\n",
    "    print(\"❗ No valid Excel files were processed.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db48f1e3",
   "metadata": {},
   "source": [
    "The extraction leads to a data frame for which the columns are a little bit deranged. In particular, the relevant columns are sometimes stored in 1 to 12 and sometimes 17 to 39, so the following code renames the columns and aligns the data frame correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34154b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Berufsgruppe', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6',\n",
       "       'col_7', 'col_8', 'col_9', 'col_10', 'col_11', 'col_12', 'Bundesland',\n",
       "       'Year', 'Month', 'source_file', 'dup_1', 'dup_2', 'dup_3', 'dup_4',\n",
       "       'dup_5', 'dup_6', 'dup_7', 'dup_8', 'dup_9', 'dup_10', 'dup_11',\n",
       "       'dup_12'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_column_names(df):\n",
    "    new_columns = []\n",
    "\n",
    "    seen = {}\n",
    "    for col in df.columns:\n",
    "        if isinstance(col, int) or (isinstance(col, str) and col.isdigit()):\n",
    "            col_str = str(col)\n",
    "            if col_str in seen:\n",
    "                new_col = f\"dup_{col_str}\"\n",
    "            else:\n",
    "                new_col = f\"col_{col_str}\"\n",
    "                seen[col_str] = True\n",
    "            new_columns.append(new_col)\n",
    "        elif col == 'Unnamed: 0':\n",
    "            new_columns.append(\"Berufsgruppe\")\n",
    "        else:\n",
    "            new_columns.append(str(col))\n",
    "\n",
    "    df.columns = new_columns\n",
    "    return df\n",
    "df = clean_column_names(combined_excel_df)\n",
    "print(df.columns)\n",
    "\n",
    "right = df.dropna(subset=['dup_1'])\n",
    "print(right.head(10))\n",
    "\n",
    "cols_to_drop = [f'col_{i}' for i in range(1, 13)]\n",
    "right = right.drop(columns=cols_to_drop)\n",
    "print(right.head(10))\n",
    "\n",
    "# Rename 'dup_1' to 'col_1', ..., 'dup_12' to 'col_12'\n",
    "rename_dict = {f'dup_{i}': f'col_{i}' for i in range(1, 13)}\n",
    "right = right.rename(columns=rename_dict)\n",
    "\n",
    "# Define the target column order\n",
    "desired_order = [\n",
    "    'Berufsgruppe', 'col_1', 'col_2', 'col_3', 'col_4', 'col_5', 'col_6',\n",
    "    'col_7', 'col_8', 'col_9', 'col_10', 'col_11', 'col_12',\n",
    "    'Bundesland', 'Year', 'Month', 'source_file'\n",
    "]\n",
    "\n",
    "# Reorder the columns to match\n",
    "right = right[desired_order]\n",
    "print(right.head(10))\n",
    "\n",
    "left = df.dropna(subset=['col_1'])\n",
    "print(left.head(10))\n",
    "\n",
    "cols_to_drop = [f'dup_{i}' for i in range(1, 13)]\n",
    "left = left.drop(columns=cols_to_drop)\n",
    "print(left.head(10))\n",
    "\n",
    "# Merge the two parts from the Excel data frame: \n",
    "\n",
    "combined_df = pd.concat([left, right], ignore_index=True)\n",
    "print(combined_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a58db26",
   "metadata": {},
   "source": [
    "Next, I clean the combined data frame (dealing with special characters, adding BKZ, ajusting column names, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b20bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the combined DataFrame\n",
    "###########################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# 1. Rename columns properly (based on your screenshots)\n",
    "combined_df.columns = [\n",
    "    \"Beruf\", \n",
    "    \"Bewerber_Anzahl_Ins\", \"Bewerber_V_Ins\",\n",
    "    \"Bewerber_Unversorgt_Anzahl\", \"Bewerber_V_Unversorgt\",\n",
    "    \"Stellen_Anzahl\", \"Stellen_V\",\n",
    "    \"Unbesetzt_Anzahl\", \"Unbesetzt_V\",\n",
    "    \"Bewerber_pro_100_Stellen\", \"Bewerber_pro_100_Stellen_V\",\n",
    "    \"Unversorgte_pro_100_Unbesetzt\", \"Unversorgte_pro_100_Unbesetzt_V\",\n",
    "    \"Bundesland\", \"Year\", \"Month\", \"source_file\"\n",
    "] \n",
    "\n",
    "# 2. Replace \"x\" and \"*\" and \"-\" with NaN\n",
    "combined_df.replace([\"x\", \"*\", \"-\", \".x\"], np.nan, inplace=True)\n",
    "\n",
    "# 3. Drop rows where Beruf is NaN or irrelevant\n",
    "combined_df = combined_df[combined_df[\"Beruf\"].notna()]\n",
    "\n",
    "# 4. Extract BKZ code (1-3 digit code at start of Beruf string)\n",
    "combined_df[\"BKZ\"] = combined_df[\"Beruf\"].astype(str).str.extract(r\"^(\\d{1,3})\")\n",
    "\n",
    "# 5. Keep only rows that have a valid BKZ\n",
    "combined_df = combined_df[combined_df[\"BKZ\"].notna()]\n",
    "\n",
    "# 6. Reset index\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# 7. Preview cleaned data\n",
    "combined_df.head(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fd9c63",
   "metadata": {},
   "source": [
    "Preview combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93e2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd91cbc",
   "metadata": {},
   "source": [
    "Summerize combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c0e6810b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Beruf</th>\n",
       "      <th>Bewerber_Anzahl_Ins</th>\n",
       "      <th>Bewerber_V_Ins</th>\n",
       "      <th>Bewerber_Unversorgt_Anzahl</th>\n",
       "      <th>Bewerber_V_Unversorgt</th>\n",
       "      <th>Stellen_Anzahl</th>\n",
       "      <th>Stellen_V</th>\n",
       "      <th>Unbesetzt_Anzahl</th>\n",
       "      <th>Unbesetzt_V</th>\n",
       "      <th>Bewerber_pro_100_Stellen</th>\n",
       "      <th>Bewerber_pro_100_Stellen_V</th>\n",
       "      <th>Unversorgte_pro_100_Unbesetzt</th>\n",
       "      <th>Unversorgte_pro_100_Unbesetzt_V</th>\n",
       "      <th>Bundesland</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>source_file</th>\n",
       "      <th>BKZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>92992</td>\n",
       "      <td>84497.000000</td>\n",
       "      <td>85226.000000</td>\n",
       "      <td>44896.000000</td>\n",
       "      <td>47631.000000</td>\n",
       "      <td>86275.000000</td>\n",
       "      <td>85918.000000</td>\n",
       "      <td>46467.000000</td>\n",
       "      <td>48191.000000</td>\n",
       "      <td>71449.000000</td>\n",
       "      <td>75162.000000</td>\n",
       "      <td>34709.000000</td>\n",
       "      <td>35237.000000</td>\n",
       "      <td>92992</td>\n",
       "      <td>92992.000000</td>\n",
       "      <td>92992.000000</td>\n",
       "      <td>92992</td>\n",
       "      <td>92992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>832</td>\n",
       "      <td>116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>111 Landwirtschaft</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ausbildungsstellenmarkt-mit-zkt-16-0-202101-xl...</td>\n",
       "      <td>111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5812</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115</td>\n",
       "      <td>832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>294.943051</td>\n",
       "      <td>-0.779236</td>\n",
       "      <td>127.593460</td>\n",
       "      <td>-1.451855</td>\n",
       "      <td>399.924323</td>\n",
       "      <td>0.912596</td>\n",
       "      <td>199.316117</td>\n",
       "      <td>1.073952</td>\n",
       "      <td>83.325111</td>\n",
       "      <td>2.916180</td>\n",
       "      <td>91.099754</td>\n",
       "      <td>1.665302</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022.481934</td>\n",
       "      <td>6.389711</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>973.365286</td>\n",
       "      <td>38.299426</td>\n",
       "      <td>417.452833</td>\n",
       "      <td>43.420283</td>\n",
       "      <td>1366.782655</td>\n",
       "      <td>37.114933</td>\n",
       "      <td>716.437190</td>\n",
       "      <td>43.196051</td>\n",
       "      <td>171.596997</td>\n",
       "      <td>76.309642</td>\n",
       "      <td>170.877291</td>\n",
       "      <td>99.298857</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.303234</td>\n",
       "      <td>3.605148</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-2380.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-3116.190476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>-15.419904</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>-18.750000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>-13.689910</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>-16.100000</td>\n",
       "      <td>1.900000</td>\n",
       "      <td>-1.306235</td>\n",
       "      <td>4.216867</td>\n",
       "      <td>-5.314371</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2021.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>51.000000</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.440000</td>\n",
       "      <td>1.120000</td>\n",
       "      <td>35.294118</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2022.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>10.294118</td>\n",
       "      <td>84.000000</td>\n",
       "      <td>10.126582</td>\n",
       "      <td>256.000000</td>\n",
       "      <td>10.710000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>91.111111</td>\n",
       "      <td>5.245395</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2024.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>26930.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>11846.000000</td>\n",
       "      <td>242.857143</td>\n",
       "      <td>28749.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>16553.000000</td>\n",
       "      <td>246.800000</td>\n",
       "      <td>3533.333333</td>\n",
       "      <td>2245.833333</td>\n",
       "      <td>3133.333333</td>\n",
       "      <td>2956.862745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2025.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Beruf  Bewerber_Anzahl_Ins  Bewerber_V_Ins  \\\n",
       "count                92992         84497.000000    85226.000000   \n",
       "unique                 116                  NaN             NaN   \n",
       "top     111 Landwirtschaft                  NaN             NaN   \n",
       "freq                   832                  NaN             NaN   \n",
       "mean                   NaN           294.943051       -0.779236   \n",
       "std                    NaN           973.365286       38.299426   \n",
       "min                    NaN             0.000000     -100.000000   \n",
       "25%                    NaN            11.000000      -15.419904   \n",
       "50%                    NaN            51.000000       -0.300000   \n",
       "75%                    NaN           188.000000       10.294118   \n",
       "max                    NaN         26930.000000      250.000000   \n",
       "\n",
       "        Bewerber_Unversorgt_Anzahl  Bewerber_V_Unversorgt  Stellen_Anzahl  \\\n",
       "count                 44896.000000           47631.000000    86275.000000   \n",
       "unique                         NaN                    NaN             NaN   \n",
       "top                            NaN                    NaN             NaN   \n",
       "freq                           NaN                    NaN             NaN   \n",
       "mean                    127.593460              -1.451855      399.924323   \n",
       "std                     417.452833              43.420283     1366.782655   \n",
       "min                       0.000000            -100.000000        0.000000   \n",
       "25%                       5.000000             -18.750000       13.000000   \n",
       "50%                      22.000000               0.000000       63.000000   \n",
       "75%                      84.000000              10.126582      256.000000   \n",
       "max                   11846.000000             242.857143    28749.000000   \n",
       "\n",
       "           Stellen_V  Unbesetzt_Anzahl   Unbesetzt_V  \\\n",
       "count   85918.000000      46467.000000  48191.000000   \n",
       "unique           NaN               NaN           NaN   \n",
       "top              NaN               NaN           NaN   \n",
       "freq             NaN               NaN           NaN   \n",
       "mean        0.912596        199.316117      1.073952   \n",
       "std        37.114933        716.437190     43.196051   \n",
       "min      -100.000000          0.000000   -100.000000   \n",
       "25%       -13.689910          6.000000    -16.100000   \n",
       "50%         0.000000         31.000000      0.000000   \n",
       "75%        10.710000        126.000000     12.000000   \n",
       "max       250.000000      16553.000000    246.800000   \n",
       "\n",
       "        Bewerber_pro_100_Stellen  Bewerber_pro_100_Stellen_V  \\\n",
       "count               71449.000000                75162.000000   \n",
       "unique                       NaN                         NaN   \n",
       "top                          NaN                         NaN   \n",
       "freq                         NaN                         NaN   \n",
       "mean                   83.325111                    2.916180   \n",
       "std                   171.596997                   76.309642   \n",
       "min                     0.000000                -2380.000000   \n",
       "25%                     1.900000                   -1.306235   \n",
       "50%                    28.440000                    1.120000   \n",
       "75%                    91.111111                    5.245395   \n",
       "max                  3533.333333                 2245.833333   \n",
       "\n",
       "        Unversorgte_pro_100_Unbesetzt  Unversorgte_pro_100_Unbesetzt_V  \\\n",
       "count                    34709.000000                     35237.000000   \n",
       "unique                            NaN                              NaN   \n",
       "top                               NaN                              NaN   \n",
       "freq                              NaN                              NaN   \n",
       "mean                        91.099754                         1.665302   \n",
       "std                        170.877291                        99.298857   \n",
       "min                          0.000000                     -3116.190476   \n",
       "25%                          4.216867                        -5.314371   \n",
       "50%                         35.294118                         0.877193   \n",
       "75%                        100.000000                         7.000000   \n",
       "max                       3133.333333                      2956.862745   \n",
       "\n",
       "       Bundesland          Year         Month  \\\n",
       "count       92992  92992.000000  92992.000000   \n",
       "unique         16           NaN           NaN   \n",
       "top            01           NaN           NaN   \n",
       "freq         5812           NaN           NaN   \n",
       "mean          NaN   2022.481934      6.389711   \n",
       "std           NaN      1.303234      3.605148   \n",
       "min           NaN   2020.000000      1.000000   \n",
       "25%           NaN   2021.000000      3.000000   \n",
       "50%           NaN   2022.000000      6.000000   \n",
       "75%           NaN   2024.000000     10.000000   \n",
       "max           NaN   2025.000000     12.000000   \n",
       "\n",
       "                                              source_file    BKZ  \n",
       "count                                               92992  92992  \n",
       "unique                                                832    116  \n",
       "top     ausbildungsstellenmarkt-mit-zkt-16-0-202101-xl...    111  \n",
       "freq                                                  115    832  \n",
       "mean                                                  NaN    NaN  \n",
       "std                                                   NaN    NaN  \n",
       "min                                                   NaN    NaN  \n",
       "25%                                                   NaN    NaN  \n",
       "50%                                                   NaN    NaN  \n",
       "75%                                                   NaN    NaN  \n",
       "max                                                   NaN    NaN  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb7de3",
   "metadata": {},
   "source": [
    "Export combined_df which is all data for 2020-2025 extracted from the excel files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e275a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv(r\"C:\\Users\\jhummels\\OneDrive - DIW Berlin\\Gehlen, Annica's files - retirement-labor-shortages\\Data\\BA_data\\B_Bewerber und Berufsausbildungsstellen\\arbeitsagentur_dataframes_Ausbildungen\\combined_excel_df.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad7975",
   "metadata": {},
   "source": [
    "Next, I extract all data before 2020 that is stored in pdf files. To extract the tables from the pdf files, I use the camelot package. Inside the camelot package, I use the network algorithm that identifies the right table alignment and stores the columns. However, the code needs a lot of time to process. Moreover, the desired tables are stored on different pages dependent on the publishing date and bland. I identified a pattern in the page number dependent on their publishing date and define specific pages based on the pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ca3126",
   "metadata": {},
   "source": [
    "The following code focuses on the specific pdf file 'ausbildungsstellenmarkt-mit-zkt-14-0-201802-pdf.pdf' to identify the right extraction method and plots the table selection of the network algorithm in the camelot package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64481321",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "\n",
    "# Path to the specific PDF file\n",
    "pdf_document = r\"C:\\Users\\jhummels\\OneDrive - DIW Berlin\\Gehlen, Annica's files - retirement-labor-shortages\\Data\\BA_data\\B_Bewerber und Berufsausbildungsstellen\\arbeitsagentur_pdfs_Ausbildungen\\ausbildungsstellenmarkt-mit-zkt-14-0-201802-pdf.pdf\"\n",
    "\n",
    "# Extract tables from pages 16 and 17\n",
    "tables = camelot.read_pdf(pdf_document, pages='8,9,10', flavor='network', strip_text=\"\\n\")\n",
    "\n",
    "print(f\"✅ Found {len(tables)} table(s)\")\n",
    "\n",
    "if len(tables) >= 2:\n",
    "    df1 = tables[0].df\n",
    "    df2 = tables[1].df\n",
    "    df3 = tables[2].df\n",
    "\n",
    "    # Combine the two tables\n",
    "    combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "    # Show and save\n",
    "    print(combined_df.head(10))\n",
    "    combined_df.to_csv(\"page16_17_combined.csv\", index=False)\n",
    "\n",
    "else:\n",
    "    print(\"❌ Less than 2 tables found, cannot combine.\")\n",
    "\n",
    "    \n",
    "    \n",
    "import matplotlib.pyplot as plt\n",
    " # Increase size and sharpness\n",
    "fig = camelot.plot(tables[2], kind ='grid')\n",
    "fig.set_size_inches(50, 50)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "xs = np.arange(0, 600, 10)\n",
    "ax = fig.gca()\n",
    "ax.set_xticks(xs)\n",
    "# ax.set_yticks(ys)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd52c74",
   "metadata": {},
   "source": [
    "The the tables extracted from the one specific pdf file are stored in the following data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0d94b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25867d08",
   "metadata": {},
   "source": [
    "The following code uses the insights from the one specific pdf file and generalizes it to all downloaded pdf files between 2014 and 2015."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8509ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the folder containing the PDF files\n",
    "pdf_folder = r\"C:\\Users\\jhummels\\OneDrive - DIW Berlin\\Gehlen, Annica's files - retirement-labor-shortages\\Data\\BA_data\\B_Bewerber und Berufsausbildungsstellen\\arbeitsagentur_pdfs_Ausbildungen\"\n",
    "\n",
    "# List to collect all DataFrames\n",
    "all_dfs = []\n",
    "\n",
    "# Loop through all PDF files in the folder\n",
    "for file in os.listdir(pdf_folder):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(pdf_folder, file)\n",
    "\n",
    "        # Extract metadata from filename\n",
    "        match = re.search(r\"ausbildungsstellenmarkt-mit-zkt-(\\d{2})-0-(\\d{6})\", file)\n",
    "        if not match:\n",
    "            print(f\"⚠️ Skipping: Filename doesn't match expected pattern: {file}\")\n",
    "            continue\n",
    "\n",
    "        bundesland = match.group(1)\n",
    "        year = int(match.group(2)[:4])\n",
    "        month = int(match.group(2)[4:])\n",
    "\n",
    "        # Filter and assign page range\n",
    "        if (year == 2014 and month >= 8) or (year == 2015 and month <= 9):\n",
    "            print(f\"✅ Processing: {file} ({bundesland}, {year}-{month:02d})\")\n",
    "            if month in [3, 7, 8, 9]:\n",
    "                page_range = \"17, 18, 19\"\n",
    "            else:\n",
    "                page_range = \"16, 17, 18\"\n",
    "        else:\n",
    "            print(f\"❌ Skipping: {file} ({bundesland}, {year}-{month:02d})\")\n",
    "            continue\n",
    "\n",
    "        # Try reading and combining tables\n",
    "        try:\n",
    "            tables = camelot.read_pdf(filepath, pages=page_range, flavor='network', strip_text=\"\\n\")\n",
    "\n",
    "            if len(tables) >= 2:\n",
    "                dfs = [tables[i].df for i in range(min(3, len(tables)))]  # Grab up to 3 tables\n",
    "                combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "                # Add metadata\n",
    "                combined_df[\"Bundesland\"] = bundesland\n",
    "                combined_df[\"Year\"] = year\n",
    "                combined_df[\"Month\"] = month\n",
    "                combined_df[\"source_file\"] = file\n",
    "\n",
    "                all_dfs.append(combined_df)\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping {file}: Less than 2 tables found.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading {file}: {e}\")\n",
    "\n",
    "# Combine and export all data\n",
    "if all_dfs:\n",
    "    final_df = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ Combined {len(all_dfs)} files. Total rows: {final_df.shape[0]}\")\n",
    "    print(final_df.head())\n",
    "    final_df.to_csv(\"combined_all_pdfs.csv\", index=False)\n",
    "else:\n",
    "    print(\"❗ No valid tables extracted from any PDFs.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b6785c",
   "metadata": {},
   "source": [
    "Export the final data set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "25c22d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv(\"combined_all_pdfs.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494210f9",
   "metadata": {},
   "source": [
    "Check summary statistics of the final data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2480e140",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427af159",
   "metadata": {},
   "source": [
    "Output preview of the final data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff1f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b876858",
   "metadata": {},
   "source": [
    "Next, I clean the data frame, including renaming columns, replacing special characters, extracting BKZ, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721e8750",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# 2. Rename columns properly (based on your screenshots)\n",
    "final_df.columns = [\n",
    "    \"Beruf\", \n",
    "    \"Bewerber_Anzahl_Ins\", \"Bewerber_V_Ins\",\n",
    "    \"Bewerber_Unversorgt_Anzahl\", \"Bewerber_V_Unversorgt\",\n",
    "    \"Ausbildungsstellen_Anzahl\", \"Ausbildungsstellen_V\",\n",
    "    \"Ausbildungsstellen_Unbesetzt_Anzahl\", \"Ausbidlungsstellen_Unbesetzt_V\",\n",
    "    \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vorjahr\", \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vor_Vorjahr\",\n",
    "    \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vorjahr\", \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vor_Vorjahr\",\n",
    "    \"Bundesland\", \"Year\", \"Month\", \"source_file\"\n",
    "] \n",
    "\n",
    "# 3. Replace \"x\" and \"*\" with NaN\n",
    "final_df.replace([\"x\", \"*\", \"-\"], np.nan, inplace=True)\n",
    "\n",
    "# 4. Drop rows where Beruf is NaN or irrelevant\n",
    "final_df = final_df[final_df[\"Beruf\"].notna()]\n",
    "\n",
    "# 5. Extract BKZ code (1-3 digit code at start of Beruf string)\n",
    "final_df[\"BKZ\"] = final_df[\"Beruf\"].astype(str).str.extract(r\"^(\\d{1,3})\")\n",
    "\n",
    "# 6. Keep only rows that have a valid BKZ\n",
    "final_df = final_df[final_df[\"BKZ\"].notna()]\n",
    "\n",
    "\n",
    "# 7. Covnert columns to numeric\n",
    "\n",
    "# List of columns to aggregate\n",
    "columns_to_aggregate = [ \n",
    "    \"Bewerber_Anzahl_Ins\", \"Bewerber_V_Ins\",\n",
    "    \"Bewerber_Unversorgt_Anzahl\", \"Bewerber_V_Unversorgt\",\n",
    "    \"Ausbildungsstellen_Anzahl\", \"Ausbildungsstellen_V\",\n",
    "    \"Ausbildungsstellen_Unbesetzt_Anzahl\", \"Ausbidlungsstellen_Unbesetzt_V\",\n",
    "    \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vorjahr\", \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vor_Vorjahr\",\n",
    "    \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vorjahr\", \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vor_Vorjahr\"\n",
    "]\n",
    "\n",
    "for col in columns_to_aggregate:\n",
    "    if col in final_df.columns:\n",
    "        final_df[col] = final_df[col].astype(str) \\\n",
    "            .str.replace('.', '', regex=False) \\\n",
    "            .str.replace(',', '.', regex=False) \\\n",
    "            .str.strip() \\\n",
    "            .apply(lambda x: float(x) if x.replace('.', '', 1).replace('-', '').isdigit() else float('nan'))\n",
    "\n",
    "# 8. Reset index\n",
    "final_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Preview cleaned data\n",
    "final_df.head(10)\n",
    "final_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19129f17",
   "metadata": {},
   "source": [
    "Next, I use the same procedure but for a different time frame (2015-2020) of pdf files for which the format is slightly different (different page numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78f84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "camelot\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Path to the folder containing the PDF files\n",
    "pdf_folder = r\"C:\\Users\\jhummels\\OneDrive - DIW Berlin\\Gehlen, Annica's files - retirement-labor-shortages\\Data\\BA_data\\B_Bewerber und Berufsausbildungsstellen\\arbeitsagentur_pdfs_Ausbildungen\"\n",
    "\n",
    "# List to collect all DataFrames\n",
    "all_dfs = []\n",
    "\n",
    "# Loop through all PDF files in the folder\n",
    "for file in os.listdir(pdf_folder):\n",
    "    if file.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(pdf_folder, file)\n",
    "\n",
    "        # Extract metadata from filename\n",
    "        match = re.search(r\"ausbildungsstellenmarkt-mit-zkt-(\\d{2})-0-(\\d{6})\", file)\n",
    "        if not match:\n",
    "            print(f\"⚠️ Skipping: Filename doesn't match expected pattern: {file}\")\n",
    "            continue\n",
    "\n",
    "        bundesland = match.group(1)\n",
    "        year = int(match.group(2)[:4])\n",
    "        month = int(match.group(2)[4:])\n",
    "\n",
    "        # Filter and assign page range\n",
    "        if ((year > 2015 and year < 2017) or (year == 2015 and month > 9) or (year == 2017 and month <= 10)):\n",
    "    \n",
    "            print(f\"✅ Processing (2015–2020): {file} ({bundesland}, {year}-{month:02d})\")\n",
    "            if month in [1, 2]:\n",
    "                page_range = \"8,9,10\"\n",
    "            else:\n",
    "                page_range = \"16,17,18\"\n",
    "        elif ((year == 2017 and month >= 11) or (year == 2018 and month <= 3)):\n",
    "            print(f\"✅ Processing (2015–2020): {file} ({bundesland}, {year}-{month:02d})\")\n",
    "            if month in [1, 2]:\n",
    "                page_range = \"9,10,11\"\n",
    "            else:\n",
    "                page_range = \"16,17,18\"\n",
    "        elif ((year == 2018 and month >= 4) or (year == 2020 and month <= 10)):\n",
    "            if month in [1, 2]:\n",
    "                page_range = \"8,9,10\"\n",
    "            else:\n",
    "                page_range = \"16,17,18\"\n",
    "        else:\n",
    "            print(f\"❌ Skipping: {file} ({bundesland}, {year}-{month:02d})\")\n",
    "            continue\n",
    "\n",
    "        # Try reading and combining tables\n",
    "        try:\n",
    "            tables = camelot.read_pdf(filepath, pages=page_range, flavor='network', strip_text=\"\\n\")\n",
    "\n",
    "            if len(tables) >= 2:\n",
    "                dfs = [table.df for table in tables]\n",
    "                # Grab up to 3 tables\n",
    "                combined_df = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "                # Add metadata\n",
    "                combined_df[\"Bundesland\"] = bundesland\n",
    "                combined_df[\"Year\"] = year\n",
    "                combined_df[\"Month\"] = month\n",
    "                combined_df[\"source_file\"] = file\n",
    "\n",
    "                all_dfs.append(combined_df)\n",
    "            else:\n",
    "                print(f\"⚠️ Skipping {file}: Less than 2 tables found.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error reading {file}: {e}\")\n",
    "\n",
    "# Combine and export all data\n",
    "if all_dfs:\n",
    "    final_df_2 = pd.concat(all_dfs, ignore_index=True)\n",
    "    print(f\"✓ Combined {len(all_dfs)} files. Total rows: {final_df_2.shape[0]}\")\n",
    "    print(final_df_2.head())\n",
    "    final_df_2.to_csv(\"combined_all_pdfs_2.csv\", index=False)\n",
    "else:\n",
    "    print(\"❗ No valid tables extracted from any PDFs.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c501380f",
   "metadata": {},
   "source": [
    "Preview the final data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d15bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_2.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f340e447",
   "metadata": {},
   "source": [
    "Look at summary statistics of the data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4420b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12',\n",
       "       '13', '14', 'Bundesland', 'Year', 'Month', 'source_file'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df_2.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b88c097",
   "metadata": {},
   "source": [
    "Export data frame to csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5dd77102",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_2 = pd.read_csv(\"combined_all_pdfs_2.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa134d9",
   "metadata": {},
   "source": [
    "Next, I clean the data frame, including renaming columns, replacing special characters, extracting BKZ, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Drop columns by their string names\n",
    "final_df_2.drop(columns=['13', '14'], inplace=True)\n",
    "\n",
    "\n",
    "# 2. Rename columns properly (based on your screenshots)\n",
    "final_df_2.columns = [\n",
    "    \"Beruf\", \n",
    "    \"Bewerber_Anzahl_Ins\", \"Bewerber_V_Ins\",\n",
    "    \"Bewerber_Unversorgt_Anzahl\", \"Bewerber_V_Unversorgt\",\n",
    "    \"Ausbildungsstellen_Anzahl\", \"Ausbildungsstellen_V\",\n",
    "    \"Ausbildungsstellen_Unbesetzt_Anzahl\", \"Ausbidlungsstellen_Unbesetzt_V\",\n",
    "    \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vorjahr\", \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vor_Vorjahr\",\n",
    "    \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vorjahr\", \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vor_Vorjahr\",\n",
    "    \"Bundesland\", \"Year\", \"Month\", \"source_file\"\n",
    "] \n",
    "\n",
    "# 3. Replace \"x\" and \"*\" with NaN\n",
    "final_df_2.replace([\"x\", \"*\", \"-\"], np.nan, inplace=True)\n",
    "\n",
    "# 4. Drop rows where Beruf is NaN or irrelevant\n",
    "final_df_2 = final_df_2[final_df_2[\"Beruf\"].notna()]\n",
    "\n",
    "# 5. Extract BKZ code (1-3 digit code at start of Beruf string)\n",
    "final_df_2[\"BKZ\"] = final_df_2[\"Beruf\"].astype(str).str.extract(r\"^(\\d{1,3})\")\n",
    "\n",
    "# 6. Keep only rows that have a valid BKZ_2\n",
    "final_df_2 = final_df_2[final_df_2[\"BKZ\"].notna()]\n",
    "\n",
    "\n",
    "# 7. Covnert columns to numeric\n",
    "\n",
    "# List of columns to aggregate\n",
    "columns_to_aggregate = [ \n",
    "    \"Bewerber_Anzahl_Ins\", \"Bewerber_V_Ins\",\n",
    "    \"Bewerber_Unversorgt_Anzahl\", \"Bewerber_V_Unversorgt\",\n",
    "    \"Ausbildungsstellen_Anzahl\", \"Ausbildungsstellen_V\",\n",
    "    \"Ausbildungsstellen_Unbesetzt_Anzahl\", \"Ausbidlungsstellen_Unbesetzt_V\",\n",
    "    \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vorjahr\", \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vor_Vorjahr\",\n",
    "    \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vorjahr\", \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vor_Vorjahr\"\n",
    "]\n",
    "\n",
    "for col in columns_to_aggregate:\n",
    "    if col in final_df_2.columns:\n",
    "        final_df_2[col] = final_df_2[col].astype(str) \\\n",
    "            .str.replace('.', '', regex=False) \\\n",
    "            .str.replace(',', '.', regex=False) \\\n",
    "            .str.strip() \\\n",
    "            .apply(lambda x: float(x) if x.replace('.', '', 1).replace('-', '').isdigit() else float('nan'))\n",
    "\n",
    "# 8. Reset index\n",
    "final_df_2.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Preview cleaned data\n",
    "final_df_2.head(10)\n",
    "final_df_2.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66109cc9",
   "metadata": {},
   "source": [
    "Now, I combine the data frame extracted from the pdf files for the two different time frames. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f8f56446",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine final_df and final_df_2   \n",
    "\n",
    "merged = pd.concat([final_df, final_df_2], ignore_index=True)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7096de7",
   "metadata": {},
   "source": [
    "Looking at summary statistics: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec493da",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.describe(include=\"all\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db08b97",
   "metadata": {},
   "source": [
    "Look at first rows of df: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e5b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b65bf4",
   "metadata": {},
   "source": [
    "Renaming columns of the pdf data frame so the fit the column names of the data frame that I obtained from the excel files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f33355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Rename columns properly \n",
    "combined_excel_df.columns = [\n",
    "    \"Beruf\", \n",
    "    \"Bewerber_Anzahl_Ins\", \"Bewerber_V_Ins\",\n",
    "    \"Bewerber_Unversorgt_Anzahl\", \"Bewerber_V_Unversorgt\",\n",
    "    \"Ausbildungsstellen_Anzahl\", \"Ausbildungsstellen_V\",\n",
    "    \"Ausbildungsstellen_Unbesetzt_Anzahl\", \"Ausbidlungsstellen_Unbesetzt_V\",\n",
    "    \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vorjahr\", \"gemeldete_Berufsausbildungsstellen_je_Bewerber_Vor_Vorjahr\",\n",
    "    \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vorjahr\", \"unbesetzte_Berufsausbildungsstellen_je_unversorgter_Bewerber_Vor_Vorjahr\",\n",
    "    \"Bundesland\", \"Year\", \"Month\", \"source_file\", \"BKZ\"\n",
    "] \n",
    "\n",
    "# Combine the two DataFrames\n",
    "\n",
    "Ausbildungsstellen = pd.concat([merged, combined_excel_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493ec50",
   "metadata": {},
   "source": [
    "Adjusting variable types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237fd7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_excel_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "9ec8f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ausbildungsstellen = Ausbildungsstellen[Ausbildungsstellen['BKZ'].astype(str).str.match(r'^\\d{3}$')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54ba045",
   "metadata": {},
   "source": [
    "Exporting final data frame as csv: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0a7e0b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ausbildungsstellen.to_csv(r\"C:\\Users\\jhummels\\OneDrive - DIW Berlin\\Gehlen, Annica's files - retirement-labor-shortages\\Data\\Ausbildungsstellen.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216224ca",
   "metadata": {},
   "source": [
    "Summary statistics of the final data frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32479be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ausbildungsstellen.describe(include=\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
