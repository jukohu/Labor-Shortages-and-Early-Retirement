{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3482fc4",
   "metadata": {},
   "source": [
    "#### Merged Data Frame of BA (Arbeitsstellen, Arbeitslose/Arbeisuchende, Ausbildungsdaten) and Renten data\n",
    "\n",
    "In this notebook I consolidate data that I previously scraped from the web and run preliminary analyses on them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596422f1",
   "metadata": {},
   "source": [
    "I start by importing my data. BA is the data from the BA about Arbeitsstellen, Rente is the Renten data you gave me, Ausbildung is the data from the BA on Ausbildungsstellen and Bewerber and Labor_Market_Tightness is the data on Arbeitsstellen, Arbeitslose and Arbeitssuchende. The difference betweeen the BA and Labor_Market_Tightness data is that the BA data includes the Vakanzzeit and the Labor_Market_Tightness data includes not only Arbeitslose but also Arbeitssuchende. Moreover, the BA data shows the data as a floating average while the other data is the exact value for that time period. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "32fd2fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max.rows', None)\n",
    "pd.set_option('display.max.columns', None)\n",
    "\n",
    "BA = pd.read_csv(r\"C:\\\\Users\\\\jhummels\\\\OneDrive - DIW Berlin\\\\Gehlen, Annica's files - retirement-labor-shortages\\\\Data\\\\complete_df_BA.csv\")\n",
    "Rente = pd.read_csv(r\"C:\\\\Users\\\\jhummels\\\\OneDrive - DIW Berlin\\\\Gehlen, Annica's files - retirement-labor-shortages\\\\Data\\\\complete_df_Rente.csv\")\n",
    "Ausbildung = pd.read_csv(r\"C:\\\\Users\\\\jhummels\\\\OneDrive - DIW Berlin\\\\Gehlen, Annica's files - retirement-labor-shortages\\\\Data\\\\Ausbildungsstellen.csv\")\n",
    "Labor_Market_Tightness = pd.read_csv(\"labor_market_tightness_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e670bbf",
   "metadata": {},
   "source": [
    "Next I need to perform some data augmention, so I can merge the respective data frames into one. This includes the aggregation of monthly data to yearly data, since the retirement is only available as yearly data. For the Labor_Market_Tightness Index, I am only interested in the ratio of vacancies (Bestand an Arbeitsstellen) and job searchers (Arbeitssuchende). This ratio has been found to be a good indicator for labor market tightness and labor shortages (Abraham et. Al., 2022). To get the yearly data I take the annual mean of the monthly data. For the variables of the BA data, I also take the annual mean of the monthly data. For Ausbildungen, I just take the value in september as the yearly value, as it represents the cumulative number of Bewerber, etc. of that Berichtsjahr. Moreover, I only include years in the merged data frame for which all composite data frames have data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29709196",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################################################################\n",
    "# Rename columns in the BA DataFrame\n",
    "##############################################################################################################\n",
    "\n",
    "Ausbildung.rename(columns={\"Bundesland\": \"bland\", \"BKZ\": \"kldb2010_3_akvs\", \"Year\": \"year\"}, inplace=True)\n",
    "BA.rename(columns={\"Bundesland\": \"bland\", \"BKZ\": \"kldb2010_3_akvs\", \"Year\": \"year\"}, inplace=True)\n",
    "Labor_Market_Tightness.rename(columns={\"Bundesland\": \"bland\", \"BKZ\": \"kldb2010_3_akvs\", \"Year\": \"year\"}, inplace=True)\n",
    "\n",
    "##############################################################################################################\n",
    "# Fix bland names\n",
    "##############################################################################################################\n",
    "\n",
    "ordered_labels =  ['1.4', '1.5', '1.6', '1.7', '1.8', '1.9', '1.1', '1.11', '1.12', '1.13', '1.14',\n",
    " '1.15', '1.16', '1.17', '1.18', '1.19']\n",
    "rename_map = {label: idx + 1 for idx, label in enumerate(sorted(ordered_labels))}\n",
    "Labor_Market_Tightness['bland'] = Labor_Market_Tightness['bland'].astype(str).str.strip().map(rename_map)\n",
    "Labor_Market_Tightness['bland'] = Labor_Market_Tightness['bland'].astype('Int64')\n",
    "\n",
    "##############################################################################################################\n",
    "# Group and aggregate to yearly data\n",
    "##############################################################################################################\n",
    "\n",
    "# Group and aggregate by taking the mean of the ratio \n",
    "valid = Labor_Market_Tightness.groupby(['year', 'bland', 'kldb2010_3_akvs']).agg(\n",
    "    ratio_mean=('V/S', 'mean'),\n",
    "    n_months=('V/S', 'count')\n",
    ").reset_index()\n",
    "valid = valid[[\"bland\", \"kldb2010_3_akvs\", \"year\", \"ratio_mean\", \"n_months\"]]\n",
    "\n",
    "\n",
    "# Group and aggregate by taking the mean of the composite columns and then taking the ratio\n",
    "\n",
    "# List of columns to aggregate\n",
    "columns_to_aggregate_tightness = [\n",
    " 'Arbeitssuchende_Bestand', 'Arbeitsstellen_Bestand'\n",
    "]\n",
    "\n",
    "# Group by 'bland', 'kldb2010_3_akvs', and 'year', and aggregate the specified columns to get one value per year\n",
    "# for each combination of 'bland' and 'kldb2010_3_akvs'\n",
    "valid_2 = Labor_Market_Tightness.groupby(['bland', 'kldb2010_3_akvs', 'year'])[columns_to_aggregate_tightness].mean().reset_index()\n",
    "\n",
    "valid_2['mean_V/S'] = valid_2['Arbeitsstellen_Bestand'] / valid_2['Arbeitssuchende_Bestand']\n",
    "\n",
    "# List of columns to aggregate\n",
    "columns_to_aggregate_BA = [\n",
    "    'Zugang', 'Zugang_V', 'Bestand', 'Bestand_V', '3_Monate_Vakant_Anteil', '3_Monate_Vakant_V_abs',\n",
    "    'abgesch_Vakanzzeit_Tage', 'abgesch_Vakanzzeit_V_abs', 'Arbeitslose',\n",
    "    'Arbeitslose_V', 'Relation', 'Relation_V'\n",
    "]\n",
    "\n",
    "# Group by 'bland', 'kldb2010_3_akvs', and 'year', and aggregate the specified columns to get one value per year\n",
    "# for each combination of 'bland' and 'kldb2010_3_akvs'\n",
    "BA_yearly = BA.groupby(['bland', 'kldb2010_3_akvs', 'year'])[columns_to_aggregate_BA].mean().reset_index()\n",
    "\n",
    "# Subset such that we only have data for September\n",
    "Ausbildung_yearly = Ausbildung[Ausbildung['Month'] == 9]\n",
    "\n",
    "##############################################################################################################\n",
    "# Merge DataFrames on 'kldb2010_3_akvs', 'bland', and 'year'\n",
    "##############################################################################################################\n",
    "\n",
    "valid['kldb2010_3_akvs'] = valid['kldb2010_3_akvs'].astype('float64')\n",
    "Ausbildung_yearly['kldb2010_3_akvs'] = Ausbildung_yearly['kldb2010_3_akvs'].astype('float64')\n",
    "\n",
    "print(\"Before merge:\", BA_yearly.shape[0])\n",
    "merged = BA_yearly.merge(Rente, on=['kldb2010_3_akvs', 'bland', 'year'], how='left')\n",
    "print(\"After Rente merge:\", merged.shape[0])\n",
    "\n",
    "merged = merged.merge(Ausbildung_yearly, on=['kldb2010_3_akvs', 'bland', 'year'], how='left')\n",
    "print(\"After Ausbildung merge:\", merged.shape[0])\n",
    "\n",
    "merged = merged.merge(valid, on=['bland', 'kldb2010_3_akvs', 'year'], how='left')\n",
    "print(\"After valid merge:\", merged.shape[0])\n",
    "\n",
    "merged = merged.merge(valid_2, on=['kldb2010_3_akvs', 'bland', 'year'], how='left')\n",
    "print(\"After valid_2 merge:\", merged.shape[0])\n",
    "\n",
    "##############################################################################################################\n",
    "# We have no Rentendata for 2011 and 2023-2025, no Ausbildungsdaten before 2014 and Rente with 63 was implemented in 2014\n",
    "##############################################################################################################\n",
    "\n",
    "filtered_merged = merged[\n",
    "    (~merged['year'].isin([2011, 2012, 2013, 2014, 2023, 2024, 2025])) \n",
    "]\n",
    "\n",
    "\n",
    "# Display the merged DataFrame\n",
    "filtered_merged.head()\n",
    "\n",
    "# Check for missing values in the merged DataFrame\n",
    "missing_values = filtered_merged.isnull().sum()\n",
    "missing_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1989a9",
   "metadata": {},
   "source": [
    "Then I add the tasks data to my merged data frame, so we can futher classify our Berufsgruppen by task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f12f7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tasks data\n",
    "tasks = pd.read_stata(r\"C:\\Users\\jhummels\\OneDrive - DIW Berlin\\Gehlen, Annica's files - retirement-labor-shortages\\Data\\BA_data\\C_Tasks_classification\\tasks_kldb2010_3.dta\")\n",
    "tasks = tasks[tasks[\"jahr\"] == 2013]\n",
    "\n",
    "# Rename kldb2010_3 column to match your merge key\n",
    "tasks.rename(columns={'kldb2010_3':'kldb2010_3_akvs'}, inplace=True)\n",
    "\n",
    "# Merge\n",
    "filtered_merged = pd.merge(\n",
    "    merged,\n",
    "    tasks[[\"kldb2010_3_akvs\", \"haupttask\"]],  # ensure only needed columns are included\n",
    "    on=\"kldb2010_3_akvs\",\n",
    "    how=\"left\"  # or \"inner\" if you only want matched rows\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3923036b",
   "metadata": {},
   "source": [
    "Getting an overview of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2610ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_merged.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe138e0",
   "metadata": {},
   "source": [
    "Exporting the merged data frame. If you want the data for all years you can just export 'merged'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "412be762",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_merged.to_csv(\n",
    "    r\"C:\\Users\\jhummels\\OneDrive - DIW Berlin\\Gehlen, Annica's files - retirement-labor-shortages\\Data\\complete_data_frame.csv\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37eebbe",
   "metadata": {},
   "source": [
    "Next, I standardize the data to make it comnparable across Bundesl√§nder and Berufsgruppen. I standardize it by indexing it using 2015 as a base year: x_i_th_year / x_base_year * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba3698f",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# Standardize the data with base year 2015\n",
    "####################################################################################\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Filter data for the base year (2015)\n",
    "base_year = 2015\n",
    "base_data = filtered_merged[filtered_merged['year'] == base_year]\n",
    "\n",
    "# Step 2: Calculate the base values for each 'bland' and each numeric column\n",
    "numeric_columns = filtered_merged.select_dtypes(include='number').columns.drop(['year', 'bland', 'kldb2010_3_akvs', 'counter'])  # Exclude 'year'\n",
    "base_values = base_data.groupby(['bland', 'kldb2010_3_akvs'])[numeric_columns].mean().reset_index()\n",
    "\n",
    "# Step 3: Merge the base values back into the main DataFrame\n",
    "indexed_data_filtered = filtered_merged.merge(base_values, on=['bland', 'kldb2010_3_akvs'], suffixes=('', '_base'))\n",
    "\n",
    "# Step 4: Calculate the indexed values for all numeric columns\n",
    "for col in numeric_columns:\n",
    "    indexed_data_filtered[f'{col}_indexed'] = (indexed_data_filtered[col] / indexed_data_filtered[f'{col}_base']) * 100\n",
    "\n",
    "# Step 5: Drop the base value columns\n",
    "indexed_data_filtered.drop(columns=[f'{col}_base' for col in numeric_columns], inplace=True)\n",
    "\n",
    "# Step 6: Add column with logarithmic values\n",
    "\n",
    "for col in numeric_columns:\n",
    "    indexed_data_filtered[f'{col}_log'] = np.log(indexed_data_filtered[col] + 1)  # Adding 1 to avoid log(0)\n",
    "\n",
    "# Step 5: Display the indexed data\n",
    "print(indexed_data_filtered.head(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787465c5",
   "metadata": {},
   "source": [
    "Next, I create lag values for each variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb5e7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new DataFrame with the lag values for each variable plot correlation heatmap \n",
    "####################################################################################\n",
    "\n",
    "# Select numeric columns to lag (excluding identifiers)\n",
    "numeric_columns = indexed_data_filtered.select_dtypes(include='number').columns.drop(['year', 'counter', 'bland', 'kldb2010_3_akvs'])\n",
    "\n",
    "# For each numeric column, create lagged columns by -1 and -2 years\n",
    "for col in numeric_columns:\n",
    "    for lag in [1, 2, 3, 4]:  # 1 = lag of 1 year, 2 = lag of 2 years\n",
    "        indexed_data_filtered[f'{col}_lag{lag}'] = (\n",
    "            indexed_data_filtered\n",
    "            .groupby(['bland', 'kldb2010_3_akvs'])[col]\n",
    "            .shift(lag)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9185660",
   "metadata": {},
   "source": [
    "Next, I export the indexed data as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cc743296",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_data_filtered.to_csv(\n",
    "    r\"C:\\Users\\jhummels\\OneDrive - DIW Berlin\\Gehlen, Annica's files - retirement-labor-shortages\\Data\\complete_data_frame_indexed.csv\",\n",
    "    index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39234ea",
   "metadata": {},
   "source": [
    "Next, I attempt to replicate the Engpassindicator (V/S) using the BA data that uses the floating average and using the no. of unemployed rather than all job serarchers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dc0ff22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jhummels\\AppData\\Local\\Temp\\ipykernel_19584\\3172809519.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  indexed_data_filtered['V/S'] = indexed_data_filtered['Bestand'] / indexed_data_filtered['Arbeitslose']\n"
     ]
    }
   ],
   "source": [
    "indexed_data_filtered['V/S'] = indexed_data_filtered['Bestand'] / indexed_data_filtered['Arbeitslose']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0af35ab",
   "metadata": {},
   "source": [
    "I find that the two indicators are correlated with a correlation coefficient of 0.51. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ec260ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.51632026]\n",
      " [0.51632026 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Drop rows where either 'V/S' or 'ratio_mean' is NaN\n",
    "subset = indexed_data_filtered[['V/S', 'ratio_mean']].dropna()\n",
    "\n",
    "# Now compute the correlation coefficient\n",
    "corr_matrix = np.corrcoef(subset['V/S'], subset['ratio_mean'])\n",
    "print(corr_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
